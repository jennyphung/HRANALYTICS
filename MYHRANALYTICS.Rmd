---
title: "Identifying Good employees at risk of leaving"
author: "Yonatan Avivi, Hiroko Okamura, Jenny Phung, Youhee Shin"
date: "February 3, 2017"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

The objective of this analytical report is to help companies identify good employees who are at risk of leaving the company. With this information, companies can allocate their finances and resources on in areas that can help in retaining good employees.  

## Analysis Process 

1. Data check and Visualization
First, we will analyze and visualize the data to get a basic understanding of the data inhand (*Human Resources Analytics by Ludovic Benistant from kaggle.com*). After obtaining a basic understanding of the data, we will check the correlation between the give attributes and interpret the data.

2. Cluster analysis and Segmentation
Second, we will segment the entire employees by using the cluster method to segement and profile the given employee pool,and observe if any certain segment of employees have a higher attrition rate than others.

3. Key drivers analysis
We will also try to analyze the key factors that are more influential in driving employees to leave their company using the classification model (tree induction).

Finally, we will recommend several business decisions based on our data analysis from above to help the company target and invest in their human resources effectively and reduce the risk and negative impact of losing high performing employees.

```{r setuplibraries, echo=FALSE, message=FALSE}
suppressWarnings(source("../INSEADAnalytics/AnalyticsLibraries/library.R"))
# Package options
suppressWarnings(ggthemr('fresh'))  # ggplot theme
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.35, results="asis")
options(knitr.kable.NA = '')
```


## 1. Data check and Visualisation

### 1.1 Load and Explore the data

First, let's load the data to use.

```{r echo=TRUE, tidy=TRUE}
ProjectData <- read.csv("./data/HR_data.csv")
ProjectData = data.matrix(ProjectData)
```

Description of the data

1. Employee satisfaction level
2. Last evaluation
3. Number of projects
4. Average monthly hours
5. Time spent at the company
6. Whether they have had a work accident
7. Whether they have had a promotion in the last 5 years
8. Department
9. Salary (1=low, 2=medium, 3=high)
10. Whether employee has left

```{r}
factor_used = c(1:8,11:21)
factor_used <- intersect(factor_used, 1:ncol(ProjectData))
ProjectDataFactor <- ProjectData[,factor_used]
ProjectDataFactor <- ProjectData <- data.matrix(ProjectDataFactor)
```

This is how the first 10 set of data (employees) look like.

```{r}
rownames(ProjectDataFactor) <- paste0("Obs.", sprintf("%02i", 1:nrow(ProjectDataFactor)))
knitr::kable(t(head(round(ProjectDataFactor, 2), 10)))
```

The data we use here have the following descriptive statistics.

```{r}
knitr::kable(round(my_summary(ProjectDataFactor), 2))
```


### 1.2 Scale the data

Here, we are standardizing the data in order to avoid having the problem of the result being driven by a few relatively large values. We will scale the data between 0 and 1. 

```{r, echo=TRUE, tidy=TRUE}
ProjectDataFactor_scaled=apply(ProjectDataFactor,2, function(r) {res = (r-min(r))/(max(r)-min(r)); res})
```

Below is the summary statistics of the scaled dataset.

```{r}
knitr::kable(round(my_summary(ProjectDataFactor_scaled), 2))
```


### 1.3 Check Correlations

The simplest way to have a first look at a dataset is to check the correlation. By doing this, we can easily see which factors have a high positive/negative correlation with leaving employees. This is different from a causality, therefore we cannot conclude that a highly correlated factor (independent variables) leads an employee to leave (dependent variable). Also, if some of the factors (independent variables) are highly correlated with each other, we could consider to group these attributes together.  

```{r echo=FALSE, tidy=TRUE}
thecorr = round(cor(ProjectDataFactor_scaled),2)
iprint.df(round(thecorr,2))
```

The most significant variable to look at is 'Satisfaction level', which is strongly negatively correlated with employees leaving. What influences the satisfaction level is not clearly indicated in the data description, but we can at least look at the correlation between Satisfaction level and the other variables to see what other variables could be related to Satisfaction level. The Satisfaction level is also negatively correlated with time spent at the company, and number of projects. This can be interpreted as 'the longer the employee has stayed at the company, the lower the level of satisfaction', which indicates that the company may be lacking in providing long term goals or visions. Being invloved in a lot of projects is also quite highly correlated to employees leaving. However, since long working hours do not have a significant correlation with attrition, we can also infer that being invloved in too many tasks, i.e. being disorganized and distracted, causes lower satisfactory level than simply having longer working hours. 

## 2. Cluster Analysis and Segmentation

### Test #1
### 2.1: Select segmentation variables and methods

We will segement the employees including all the variables except the variable "Whether employee has left." We will use Euclidean distance.

```{r, echo=TRUE, tidy=TRUE}
segmentation_attributes_used = c(1:6,8:19)
profile_attributes_used = c(1:19) 
numb_clusters_used = 5
profile_with = "hclust"
distance_used = "euclidean"
hclust_method = "ward.D"
```

```{r}
segmentation_attributes_used <- intersect(segmentation_attributes_used, 1:ncol(ProjectDataFactor))
profile_attributes_used <- intersect(profile_attributes_used, 1:ncol(ProjectDataFactor))

ProjectData_segment <- ProjectDataFactor_scaled[,segmentation_attributes_used]
ProjectData_profile <- ProjectDataFactor_scaled[,profile_attributes_used]
```

Here are the differences between the observations using the distance metric we selected (euclidean):

```{r}
euclidean_pairwise <- as.matrix(dist(head(ProjectData_segment, 10), method="euclidean"))
euclidean_pairwise <- euclidean_pairwise*lower.tri(euclidean_pairwise) + euclidean_pairwise*diag(euclidean_pairwise) + 10e10*upper.tri(euclidean_pairwise)
euclidean_pairwise[euclidean_pairwise==10e10] <- NA
rownames(euclidean_pairwise) <- colnames(euclidean_pairwise) <- sprintf("Obs.%02d", 1:10)

iprint.df(round(euclidean_pairwise,2))
```


### 2.2 Visualize Pair-wise Distances

Below is the histogram of, say, the first 2 variables.

```{r}
do.call(grid.arrange, lapply(1:2, function(n) {
  qplot(ProjectData_segment[, n], xlab=paste("Histogram of Variable", n), ylab="Frequency", binwidth=0.2)
}))
```

or the histogram of all pairwise distances for the `r distance_used` distance:

```{r}
Pairwise_Distances <- dist(ProjectData_segment, method = distance_used) 
qplot(as.vector(Pairwise_Distances), xlab="Histogram of all pairwise Distances between observtions", ylab="Frequency", binwidth=0.2)
```

The mountain and valley in our histogram shows us that there is a high possibility of multiple segments within the employees. We will try to identify these segments in the next part of our analysis.

### 2.3 Number of Segments

Let's use the Hierarchical Clustering methods. It may be useful to see the dendrogram from, to have a quick idea of how the data may be segmented and how many segments there may be. Here is the dendrogram for our data:

```{r}
Hierarchical_Cluster_distances <- dist(ProjectData_segment, method=distance_used)
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method=hclust_method)
# Display dendogram
iplot.dendrogram(Hierarchical_Cluster)
```

We can also plot the "distances" traveled before we merge any of the lower and smaller in size clusters into larger ones - the heights of the tree branches that link the clusters as we traverse the tree from its leaves to its root. If we have n observations, the plot will have n-1 numbers. We can see the first 20 here.

```{r}
num <- nrow(ProjectData) - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
iplot.df(melt(head(df1, 20), id="index"), xlab="Number of Components")
```

For now, let's consider the 4-segments solution. We can also see the segment each observation (respondent in this case) belongs to for the first 20 people:

```{r}
cluster_memberships_hclust <- as.vector(cutree(Hierarchical_Cluster, 4))
cluster_ids_hclust = unique(cluster_memberships_hclust)

ProjectData_with_hclust_membership <- cbind(1:length(cluster_memberships_hclust),cluster_memberships_hclust)
colnames(ProjectData_with_hclust_membership)<-c("Observation Number","Cluster_Membership")

knitr::kable(round(head(ProjectData_with_hclust_membership, 20), 2))
```

### 2.4 Profile and interpret the segments

Having decided how many clusters to use, we would like to get a better understanding of who the customers in those clusters are and interpret the segments. 

Let's see first how many observations we have in each segment, for the segments we selected above:

```{r}
cluster_memberships <- cluster_memberships_hclust
cluster_ids <-  cluster_ids_hclust

cluster_size = NULL
for (i in sort(unique(cluster_memberships))){
  cluster_size = c(cluster_size,sum(cluster_memberships == i))
  }
cluster_size = matrix(cluster_size, nrow=1)
colnames(cluster_size) <- paste("Segment", 1:length(cluster_size), sep=" ")
rownames(cluster_size) <- "Number of Obs."
iprint.df(cluster_size, scale=TRUE)
```

The average values of our data for the total population as well as within each customer segment are:

```{r}
NewData = matrix(cluster_memberships,ncol=1)

population_average = matrix(apply(ProjectData_profile, 2, mean), ncol=1)
colnames(population_average) <- "Population"
Cluster_Profile_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_profile[(cluster_memberships==i), ], 2, mean))
if (ncol(ProjectData_profile) <2)
  Cluster_Profile_mean=t(Cluster_Profile_mean)
colnames(Cluster_Profile_mean) <- paste("Segment", 1:length(cluster_ids), sep=" ")
cluster.profile <- cbind (population_average,Cluster_Profile_mean)

knitr::kable(round(cluster.profile, 2))
```

Looking at the data, we realized our segments have been divided highly depending on the department employees belong to (The employees from same departments were all grouped in the same segment). Therefore, we will re-segment our data excluding departments, and only use the 'departments' for profiling.

### Test #2
### 2.1 Select segmentation variables and methods

This is our 2nd try on segmentation - using the same method as above, but now with the variable 'departments' removed as well as "Whether employee has left". We will use Euclidean distance.

```{r, echo=TRUE, tidy=TRUE}
segmentation_attributes_used = c(1:6,8:9)
profile_attributes_used = c(1:19)
numb_clusters_used = 5
profile_with = "hclust"
distance_used = "euclidean"
hclust_method = "ward.D"
```

```{r}
segmentation_attributes_used <- intersect(segmentation_attributes_used, 1:ncol(ProjectDataFactor))
profile_attributes_used <- intersect(profile_attributes_used, 1:ncol(ProjectDataFactor))

ProjectData_segment <- ProjectDataFactor_scaled[,segmentation_attributes_used]
ProjectData_profile <- ProjectDataFactor_scaled[,profile_attributes_used]
```


### 2.2 Visualize Pair-wise Distances

We will skip this subsection for our 2nd try.

### 2.3 Number of Segments

Let's plot the "distances" between clusters before we merge any of the lower and smaller sized clusters into larger ones.

```{r}
Hierarchical_Cluster_distances <- dist(ProjectData_segment, method=distance_used)
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method=hclust_method)

num <- nrow(ProjectData) - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
iplot.df(melt(head(df1, 20), id="index"), xlab="Number of Components")
```

For now, we will choose 5 segments. We can see the segment each observation (respondent in this case) belongs to for the first 20 people:

```{r}
cluster_memberships_hclust <- as.vector(cutree(Hierarchical_Cluster, 5))
cluster_ids_hclust = unique(cluster_memberships_hclust)

ProjectData_with_hclust_membership <- cbind(1:length(cluster_memberships_hclust),cluster_memberships_hclust)
colnames(ProjectData_with_hclust_membership)<-c("Observation Number","Cluster_Membership")

knitr::kable(round(head(ProjectData_with_hclust_membership, 20), 2))
```

### 2.4 Profile and interpret the segments

Having decided how many clusters to use, we would like to have a better understanding of who the customers in those clusters are and interpret the segments.

Let's see first how many observations we have in each segment, for the segments we selected above:

```{r}
cluster_memberships <- cluster_memberships_hclust
cluster_ids <-  cluster_ids_hclust

cluster_size = NULL
for (i in sort(unique(cluster_memberships))){
  cluster_size = c(cluster_size,sum(cluster_memberships == i))
  }
cluster_size = matrix(cluster_size, nrow=1)
colnames(cluster_size) <- paste("Segment", 1:length(cluster_size), sep=" ")
rownames(cluster_size) <- "Number of Obs."
iprint.df(cluster_size, scale=TRUE)
```

The average values of our data for the total population as well as within each customer segment are:

```{r}
NewData = matrix(cluster_memberships,ncol=1)

population_average = matrix(apply(ProjectData_profile, 2, mean), ncol=1)
colnames(population_average) <- "Population"
Cluster_Profile_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_profile[(cluster_memberships==i), ], 2, mean))
if (ncol(ProjectData_profile) <2)
  Cluster_Profile_mean=t(Cluster_Profile_mean)
colnames(Cluster_Profile_mean) <- paste("Segment", 1:length(cluster_ids), sep=" ")
cluster.profile <- cbind (population_average,Cluster_Profile_mean)

knitr::kable(round(cluster.profile, 2))
```

Segment 1 to 4 has quite clear differentiation among segments, and we were able to profile them as 'Quitters', 'Pampered Loyals', 'The Burned-outs', and 'Neglected Loyals'. However, everyone in Segment 5 has had a work accident (coefficient 1), which is a variable we consider not to be much meaningful in exploring who and how to retain.
Therefore, we will redo the analysis again excluding work accident as a variable.


### Test #3
### 2.1 Select segmentation variables and methods

We will now use the variables except 'Whether employee has left','Department', and 'Work accident'. We will use Euclidean distance, just like before.

```{r, echo=TRUE, tidy=TRUE}
segmentation_attributes_used = c(1:5,8:9)
profile_attributes_used = c(1:19)
numb_clusters_used = 4
profile_with = "hclust"
distance_used = "euclidean"
hclust_method = "ward.D"
```

```{r}
segmentation_attributes_used <- intersect(segmentation_attributes_used, 1:ncol(ProjectDataFactor))
profile_attributes_used <- intersect(profile_attributes_used, 1:ncol(ProjectDataFactor))

ProjectData_segment <- ProjectDataFactor_scaled[,segmentation_attributes_used]
ProjectData_profile <- ProjectDataFactor_scaled[,profile_attributes_used]
```


### 2.2 Visualize Pair-wise Distances

We will skip this subsection for our 3rd try.

### 2.3 Number of Segments

Let's plot the "distances" between clusters before we merge any of the lower and smaller sized clusters into larger ones.

```{r}
Hierarchical_Cluster_distances <- dist(ProjectData_segment, method=distance_used)
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method=hclust_method)

num <- nrow(ProjectData) - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
iplot.df(melt(head(df1, 20), id="index"), xlab="Number of Components")
```

The appropriate number of segments is 4, with the distance between clusters dropping drastically after 4. Below are the segments assigned to each employee, for the first 20 employees (observations).

```{r}
cluster_memberships_hclust <- as.vector(cutree(Hierarchical_Cluster, 4))
cluster_ids_hclust = unique(cluster_memberships_hclust)

ProjectData_with_hclust_membership <- cbind(1:length(cluster_memberships_hclust),cluster_memberships_hclust)
colnames(ProjectData_with_hclust_membership)<-c("Observation Number","Cluster_Membership")

knitr::kable(round(head(ProjectData_with_hclust_membership, 20), 2))
```

### 2.4 Profile and interpret the segments

The number and segmentation of the clustuers seem reasonable. Now, in order to get a better understanding of who the employyes in those clusters are, we will attempt to profile and interpret the segments using the all the variables (attributes) we originally had, including the variables we excluded for segmentation.

Let's first see how many observations we have in each segment, for the segments we selected above:

```{r}
cluster_memberships <- cluster_memberships_hclust
cluster_ids <-  cluster_ids_hclust

cluster_size = NULL
for (i in sort(unique(cluster_memberships))){
  cluster_size = c(cluster_size,sum(cluster_memberships == i))
  }
cluster_size = matrix(cluster_size, nrow=1)
colnames(cluster_size) <- paste("Segment", 1:length(cluster_size), sep=" ")
rownames(cluster_size) <- "Number of Obs."
iprint.df(cluster_size, scale=TRUE)
```

The average values of our data for the total population as well as within each customer segment are as below:

```{r}
NewData = matrix(cluster_memberships,ncol=1)

population_average = matrix(apply(ProjectData_profile, 2, mean), ncol=1)
colnames(population_average) <- "Population"
Cluster_Profile_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_profile[(cluster_memberships==i), ], 2, mean))
if (ncol(ProjectData_profile) <2)
  Cluster_Profile_mean=t(Cluster_Profile_mean)
colnames(Cluster_Profile_mean) <- paste("Segment", 1:length(cluster_ids), sep=" ")
cluster.profile <- cbind (population_average,Cluster_Profile_mean)

knitr::kable(round(cluster.profile, 2))
```

### 2.5 Implications

Yonni

## 3. Drivers of Leaving Company

Yonni

### 3.1 Classification tree

Hiroko


```{r setupclassification, echo=TRUE, tidy=TRUE}
dependent_variable = 7
independent_variables = c(1:5,8:9)

Probability_Threshold = 0.5

estimation_data_percent = 80
validation_data_percent = 10

random_sampling = 0

# Tree (CART) complexity control cp
CART_cp = 0.02

# the minimum size of a segment for the analysis to be done
min_segment = 100
```

```{r}
dependent_variable = unique(sapply(dependent_variable,function(i) min(ncol(ProjectDataFactor_scaled), max(i,1))))
independent_variables = unique(sapply(independent_variables,function(i) min(ncol(ProjectDataFactor_scaled), max(i,1))))

test_data_percent = 100 - estimation_data_percent - validation_data_percent
CART_control = rpart.control(cp = CART_cp)
```

```{r}
# First we split the data in estimation, validation, and test

if (random_sampling){
  estimation_data_ids=sample.int(nrow(ProjectDataFactor_scaled),floor(estimation_data_percent*nrow(ProjectDataFactor_scaled)/100))
  non_estimation_data = setdiff(1:nrow(ProjectDataFactor_scaled),estimation_data_ids)
  validation_data_ids=non_estimation_data[sample.int(length(non_estimation_data), floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))]
  } else {
    estimation_data_ids=1:floor(estimation_data_percent*nrow(ProjectDataFactor_scaled)/100)
    non_estimation_data = setdiff(1:nrow(ProjectDataFactor_scaled),estimation_data_ids)
    validation_data_ids = (tail(estimation_data_ids,1)+1):(tail(estimation_data_ids,1) + floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))
    }

test_data_ids = setdiff(1:nrow(ProjectDataFactor_scaled), union(estimation_data_ids,validation_data_ids))

estimation_data=ProjectDataFactor_scaled[estimation_data_ids,]
validation_data=ProjectDataFactor_scaled[validation_data_ids,]
test_data=ProjectDataFactor_scaled[test_data_ids,]
```

```{r}
independent_variables_nolabel = colnames(ProjectDataFactor[,independent_variables])

estimation_data_nolabel = cbind(estimation_data[,dependent_variable], estimation_data[,independent_variables])
colnames(estimation_data_nolabel) <- c(colnames(estimation_data)[dependent_variable],independent_variables_nolabel)

validation_data_nolabel = cbind(validation_data[,dependent_variable], validation_data[,independent_variables])
colnames(validation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

test_data_nolabel = cbind(test_data[,dependent_variable], test_data[,independent_variables])
colnames(test_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

estimation_data_nolabel = data.frame(estimation_data_nolabel)
validation_data_nolabel = data.frame(validation_data_nolabel)
test_data_nolabel = data.frame(test_data_nolabel)

estimation_data = data.frame(estimation_data)
validation_data = data.frame(validation_data)
test_data = data.frame(test_data)

formula=paste(colnames(estimation_data)[dependent_variable],paste(Reduce(paste,sapply(head(independent_variables_nolabel,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables_nolabel,1),sep=""),sep="~")
CART_tree<-rpart(formula, data=estimation_data_nolabel, method="class", control=CART_control)

rpart.plot(CART_tree, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
```

```{r}
CART_tree_large<-rpart(formula, data=estimation_data_nolabel, method="class", control=rpart.control(cp = 0.005))

rpart.plot(CART_tree_large, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
```

=======
Yonni

## 4. Business Decisions

Jenny

## 5. Future Work

*I started writing this below but feel free to add, change, or delete, since I don't know what will come in 3 and 4!

First, companies can implement policies to control attrition rates, by managing the variables that have a high correlation with employees leaving. For example, companies could work to increase the satisfactory level of employees, especially in the variables that lead to low satisfactory level. To encourage the employees to stay longer, companies could share clear long term visions of the company to help employees envision their future together with the company. Companies could also implement strict working policies where employees would be restricted from joining above a certain number of projects, allowing them to deeply focus on only a few projects and therefore find more meaningful value and satisfaction. 

Second, companies can use the prediction model to retain the high performing employees with high risk of leaving.
