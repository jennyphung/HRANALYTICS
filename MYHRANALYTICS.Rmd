---
title: "Identifying Good employees at risk of leaving"
author: "Yonatan Avivi, Hiroko Okamura, Jenny Phung, Youhee Shin"
date: "January 25, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objective

The objective of this analytical report is to help companies identify good employees who are at risk of leaving the company. With this information, companies can allocate their finances and resources on in areas that can help in retaining good employees.  


# Analysis Process 

First, we will analyze and visualize the data to get a basic understanding of the data inhand (Human Resources Analytics by Ludovic Benistant from kaggle.com). After basic understanding of the data, we will check the correlation of the factors to identify and interpret the key factors that drive employees to leave. 

Second, we will segment the employees across two dimensions + add more

Finally, we will bucket the employees who have stayed across two dimensions, performance and risk of leaving, in order to predict and identify the characteristics of employees companies generally wish to retain even at a higher cost - high performing employees with high risk of leaving (and maybe even identify the low performing employees with low possiblity of leaving). This will help the company to target and invest in their human resources and reduce the risk and negative impact of losing high performing employees.


```{r setuplibraries, echo=FALSE, message=FALSE}
suppressWarnings(source("../INSEADAnalytics/AnalyticsLibraries/library.R"))
# Package options
suppressWarnings(ggthemr('fresh'))  # ggplot theme
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.35, results="asis")
options(knitr.kable.NA = '')
```


# 1. Data check and Visualisation


## 1.1 Load and Explore the Data

First we load the data to use.

```{r echo=TRUE, tidy=TRUE}
ProjectData <- read.csv("./data/HR_data.csv")
ProjectData = data.matrix(ProjectData)
```

Description of data
- Employee satisfaction level
- Last evaluation
- Number of projects
- Average monthly hours
- Time spent at the company
- Whether they have had a work accident
- Whether they have had a promotion in the last 5 years
- Department
- Salary (1=low, 2=medium, 3=high)
- Whether the employee has left

```{r}
factor_used = c(1:8,11:21)
factor_used <- intersect(factor_used, 1:ncol(ProjectData))
ProjectDataFactor <- ProjectData[,factor_used]
ProjectDataFactor <- ProjectData <- data.matrix(ProjectDataFactor)
```

This is how the first 10 data look.

```{r}
rownames(ProjectDataFactor) <- paste0("Obs.", sprintf("%02i", 1:nrow(ProjectDataFactor)))
knitr::kable(t(head(round(ProjectDataFactor, 2), 10)))
```

The data we use here have the following descriptive statistics.

```{r}
knitr::kable(round(my_summary(ProjectDataFactor), 2))
```


## 1.2 Scale the data

```{r, echo=TRUE, tidy=TRUE}
ProjectDataFactor_scaled=apply(ProjectDataFactor,2, function(r) {res = (r-min(r))/(max(r)-min(r)); res})
```

Notice now the summary statistics of the scaled dataset:

```{r}
knitr::kable(round(my_summary(ProjectDataFactor_scaled), 2))
```


## 1.3 Check Correlations

This is the correlation matrix.

```{r echo=FALSE, tidy=TRUE}
thecorr = round(cor(ProjectDataFactor_scaled),2)
iprint.df(round(thecorr,2))
```


## 2. Cluster Analysis and Segmentation

## 2.1 Select segmentation variables and methods

We use all the variables except "Whether the employee has left." We use Euclidean distance.

```{r setupcluster, echo=TRUE, tidy=TRUE}
segmentation_attributes_used = c(1:6,8,11:21)
#profile_attributes_used = c(2:82) 
numb_clusters_used = 7
profile_with = "hclust"
distance_used = "euclidean"
hclust_method = "ward.D"
```

```{r}
segmentation_attributes_used <- intersect(segmentation_attributes_used, 1:ncol(ProjectDataFactor))
#profile_attributes_used <- intersect(profile_attributes_used, 1:ncol(ProjectData))

ProjectData_segment <- ProjectDataFactor_scaled[,segmentation_attributes_used]
#ProjectData_profile <- ProjectData[,profile_attributes_used]

```

Here are the differences between the observations using the distance metric we selected:

```{r}
euclidean_pairwise <- as.matrix(dist(head(ProjectData_segment, 10), method="euclidean"))
euclidean_pairwise <- euclidean_pairwise*lower.tri(euclidean_pairwise) + euclidean_pairwise*diag(euclidean_pairwise) + 10e10*upper.tri(euclidean_pairwise)
euclidean_pairwise[euclidean_pairwise==10e10] <- NA
rownames(euclidean_pairwise) <- colnames(euclidean_pairwise) <- sprintf("Obs.%02d", 1:10)

iprint.df(round(euclidean_pairwise,2))
```


## 2.2 Visualize Pair-wise Distances

We can see the histogram of, say, the first 2 variables.

```{r}
do.call(grid.arrange, lapply(1:2, function(n) {
  qplot(ProjectData_segment[, n], xlab=paste("Histogram of Variable", n), ylab="Frequency", binwidth=0.2)
}))
```

or the histogram of all pairwise distances for the `r distance_used` distance:

```{r}
Pairwise_Distances <- dist(ProjectData_segment, method = distance_used) 
qplot(as.vector(Pairwise_Distances), xlab="Histogram of all pairwise Distances between observtions", ylab="Frequency", binwidth=0.2)
```


## 2.3 Number of Segments

Let's use Hierarchical Clustering methods. It may be useful to see the dendrogram from , to have a quick idea of how the data may be segmented and how many segments there may be. Here is the dendrogram for our data:

```{r}
Hierarchical_Cluster_distances <- dist(ProjectData_segment, method=distance_used)
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method=hclust_method)
# Display dendogram
ggdendrogram(Hierarchical_Cluster, theme_dendro=FALSE) + xlab("Our Observations") + ylab("Height")
```

We can also plot the "distances" traveled before we need to merge any of the lower and smaller in size clusters into larger ones - the heights of the tree branches that link the clusters as we traverse the tree from its leaves to its root. If we have n observations, this plot has n-1 numbers, we see the first 20 here.

```{r}
num <- nrow(ProjectData) - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
ggplot(df1, aes(x=index, y=distances)) + geom_line() + xlab("Number of Components") +ylab("Distances")
```


## 2.4 Profile and interpret the segments

## 2.6 Robustness analysis



# 3. Drivers of Leaving Company

### 3.1 Classification tree
### 3.2 Profit curve

## 4. Business Decisions


# Conclusion 
