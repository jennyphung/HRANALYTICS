---
title: "Identifying Good employees at risk of leaving"
author: "Yonatan Avivi, Hiroko Okamura, Jenny Phung, Youhee Shin"
date: "February 3, 2017"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

When salary discussions are approaching, most companies face the same challenge; how to distribute the limited cash pie among employees. Different companies developed different methods and criteria towards this challenge, however, the goal is common for most of them: reward good employees who are likely to stay with the company for the long term. The first part of this equation is quite simple: identifying the good employees. Most companies maintain evaluation process that allows them to identify high preforming employees. The latter part, however, is trickier. How can companies predict who are the employees that are likely to leave? What are the characteristics of those high-risk employees?

The goal of our analysis is to use big data analytics (~15,000 employee records) to identify groups of employees with high likelihood of leaving the company. Once these groups were identified we wish to understand the underlying drivers behind their attrition. With this kind of data in hand, companies can better allocate their resources and invest in employees in risk. As not fair as it may sound, investing resources in good employees with low likelihood to leave is a low ROI investment. 


## Analysis Process 

1. Data check and Visualization
First, we will analyze and visualize the data to get a basic understanding of the data inhand (*Human Resources Analytics by Ludovic Benistant from kaggle.com*). After obtaining a basic understanding of the data, we will check the correlation between the give attributes and interpret the data.

2. Cluster analysis and Segmentation
Second, we will segment the entire employees by using the cluster method to segement and profile the given employee pool,and observe if any certain segment of employees have a higher attrition rate than others.

3. Key drivers analysis
We will also try to analyze the key factors that are more influential in driving employees to leave their company using the classification model (tree induction).

4. Finally, we will recommend several business decisions based on our data analysis from above to help the company target and invest in their human resources effectively and reduce the risk and negative impact of losing high performing employees.


```{r setuplibraries, echo=FALSE, message=FALSE}
suppressWarnings(source("../INSEADAnalytics/AnalyticsLibraries/library.R"))
# Package options
suppressWarnings(ggthemr('fresh'))  # ggplot theme
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.35, results="asis")
options(knitr.kable.NA = '')
```

-----

## 1. Data check and Visualisation

### 1.1 Load and Explore the data

First, let's load the data to use.

```{r echo=TRUE, tidy=TRUE}
ProjectData <- read.csv("./data/HR_data.csv")
ProjectData = data.matrix(ProjectData)
```

Description of the data

1. Employee satisfaction level
2. Last evaluation
3. Number of projects
4. Average monthly hours
5. Time spent at the company
6. Whether they have had a work accident
7. Whether they have had a promotion in the last 5 years
8. Department
9. Salary (1=low, 2=medium, 3=high)
10. Whether employee has left

```{r}
factor_used = c(1:8,11:21)
factor_used <- intersect(factor_used, 1:ncol(ProjectData))
ProjectDataFactor <- ProjectData[,factor_used]
ProjectDataFactor <- ProjectData <- data.matrix(ProjectDataFactor)
```

This is how the first 10 set of data (employees) look like.

```{r}
rownames(ProjectDataFactor) <- paste0("Obs.", sprintf("%02i", 1:nrow(ProjectDataFactor)))
knitr::kable(t(head(round(ProjectDataFactor, 2), 10)))
```

The data we use here have the following descriptive statistics.

```{r}
knitr::kable(round(my_summary(ProjectDataFactor), 2))
```


### 1.2 Scale the data

Here, we are standardizing the data in order to avoid having the problem of the result being driven by a few relatively large values. We will scale the data between 0 and 1. 

```{r, echo=TRUE, tidy=TRUE}
ProjectDataFactor_scaled=apply(ProjectDataFactor,2, function(r) {res = (r-min(r))/(max(r)-min(r)); res})
```

Below is the summary statistics of the scaled dataset.

```{r}
knitr::kable(round(my_summary(ProjectDataFactor_scaled), 2))
```


### 1.3 Check Correlations

The simplest way to have a first look at a dataset is to check the correlation. By doing this, we can easily see which factors have a high positive/negative correlation with leaving employees. This is different from a causality, therefore we cannot conclude that a highly correlated factor (independent variables) leads an employee to leave (dependent variable). Also, if some of the factors (independent variables) are highly correlated with each other, we could consider to group these attributes together.  

```{r echo=FALSE, tidy=TRUE}
thecorr = round(cor(ProjectDataFactor_scaled),2)
iprint.df(round(thecorr,2))
```

The most significant variable to look at is 'Satisfaction level', which is strongly negatively correlated with employees leaving. What influences the satisfaction level is not clearly indicated in the data description, but we can at least look at the correlation between Satisfaction level and the other variables to see what other variables could be related to Satisfaction level. The Satisfaction level is also negatively correlated with time spent at the company, and number of projects. This can be interpreted as 'the longer the employee has stayed at the company, the lower the level of satisfaction', which indicates that the company may be lacking in providing long term goals or visions. Being invloved in a lot of projects is also quite highly correlated to employees leaving. However, since long working hours do not have a significant correlation with attrition, we can also infer that being invloved in too many tasks, i.e. being disorganized and distracted, causes lower satisfactory level than simply having longer working hours. 

-----

## 2. Cluster Analysis and Segmentation

### Test #1
### 2.1: Select segmentation variables and methods

We will segement the employees including all the variables except the variable "Whether employee has left." We will use Euclidean distance.

```{r, echo=TRUE, tidy=TRUE}
segmentation_attributes_used = c(1:6,8:19)
profile_attributes_used = c(1:19) 
numb_clusters_used = 5
profile_with = "hclust"
distance_used = "euclidean"
hclust_method = "ward.D"
```

```{r}
segmentation_attributes_used <- intersect(segmentation_attributes_used, 1:ncol(ProjectDataFactor))
profile_attributes_used <- intersect(profile_attributes_used, 1:ncol(ProjectDataFactor))

ProjectData_segment <- ProjectDataFactor_scaled[,segmentation_attributes_used]
ProjectData_profile <- ProjectDataFactor_scaled[,profile_attributes_used]
```

Here are the differences between the observations using the distance metric we selected (euclidean):

```{r}
euclidean_pairwise <- as.matrix(dist(head(ProjectData_segment, 10), method="euclidean"))
euclidean_pairwise <- euclidean_pairwise*lower.tri(euclidean_pairwise) + euclidean_pairwise*diag(euclidean_pairwise) + 10e10*upper.tri(euclidean_pairwise)
euclidean_pairwise[euclidean_pairwise==10e10] <- NA
rownames(euclidean_pairwise) <- colnames(euclidean_pairwise) <- sprintf("Obs.%02d", 1:10)

iprint.df(round(euclidean_pairwise,2))
```


### 2.2 Visualize Pair-wise Distances

Below is the histogram of, say, the first 2 variables.

```{r}
do.call(grid.arrange, lapply(1:2, function(n) {
  qplot(ProjectData_segment[, n], xlab=paste("Histogram of Variable", n), ylab="Frequency", binwidth=0.2)
}))
```

or the histogram of all pairwise distances for the `r distance_used` distance:

```{r}
Pairwise_Distances <- dist(ProjectData_segment, method = distance_used) 
qplot(as.vector(Pairwise_Distances), xlab="Histogram of all pairwise Distances between observtions", ylab="Frequency", binwidth=0.2)
```

The mountain and valley in our histogram shows us that there is a high possibility of multiple segments within the employees. We will try to identify these segments in the next part of our analysis.

### 2.3 Number of Segments

Let's use the Hierarchical Clustering methods. It may be useful to see the dendrogram from, to have a quick idea of how the data may be segmented and how many segments there may be. Here is the dendrogram for our data:

```{r}
Hierarchical_Cluster_distances <- dist(ProjectData_segment, method=distance_used)
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method=hclust_method)
# Display dendogram
iplot.dendrogram(Hierarchical_Cluster)
```

We can also plot the "distances" traveled before we merge any of the lower and smaller in size clusters into larger ones - the heights of the tree branches that link the clusters as we traverse the tree from its leaves to its root. If we have n observations, the plot will have n-1 numbers. We can see the first 20 here.

```{r}
num <- nrow(ProjectData) - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
iplot.df(melt(head(df1, 20), id="index"), xlab="Number of Components")
```

For now, let's consider the 4-segments solution. We can also see the segment each observation (respondent in this case) belongs to for the first 20 people:

```{r}
cluster_memberships_hclust <- as.vector(cutree(Hierarchical_Cluster, 4))
cluster_ids_hclust = unique(cluster_memberships_hclust)

ProjectData_with_hclust_membership <- cbind(1:length(cluster_memberships_hclust),cluster_memberships_hclust)
colnames(ProjectData_with_hclust_membership)<-c("Observation Number","Cluster_Membership")

knitr::kable(round(head(ProjectData_with_hclust_membership, 20), 2))
```

### 2.4 Profile and interpret the segments

Having decided how many clusters to use, we would like to get a better understanding of who the customers in those clusters are and interpret the segments. 

Let's see first how many observations we have in each segment, for the segments we selected above:

```{r}
cluster_memberships <- cluster_memberships_hclust
cluster_ids <-  cluster_ids_hclust

cluster_size = NULL
for (i in sort(unique(cluster_memberships))){
  cluster_size = c(cluster_size,sum(cluster_memberships == i))
  }
cluster_size = matrix(cluster_size, nrow=1)
colnames(cluster_size) <- paste("Segment", 1:length(cluster_size), sep=" ")
rownames(cluster_size) <- "Number of Obs."
iprint.df(cluster_size, scale=TRUE)
```

The average values of our data for the total population as well as within each customer segment are:

```{r}
NewData = matrix(cluster_memberships,ncol=1)

population_average = matrix(apply(ProjectData_profile, 2, mean), ncol=1)
colnames(population_average) <- "Population"
Cluster_Profile_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_profile[(cluster_memberships==i), ], 2, mean))
if (ncol(ProjectData_profile) <2)
  Cluster_Profile_mean=t(Cluster_Profile_mean)
colnames(Cluster_Profile_mean) <- paste("Segment", 1:length(cluster_ids), sep=" ")
cluster.profile <- cbind (population_average,Cluster_Profile_mean)

knitr::kable(round(cluster.profile, 2))
```

Looking at the data, we realized our segments have been divided highly depending on the department employees belong to (The employees from same departments were all grouped in the same segment). Therefore, we will re-segment our data excluding departments, and only use the 'departments' for profiling.

-----

### Test #2
### 2.1 Select segmentation variables and methods

This is our 2nd try on segmentation - using the same method as above, but now with the variable 'departments' removed as well as "Whether employee has left". We will use Euclidean distance.

```{r, echo=TRUE, tidy=TRUE}
segmentation_attributes_used = c(1:6,8:9)
profile_attributes_used = c(1:19)
numb_clusters_used = 5
profile_with = "hclust"
distance_used = "euclidean"
hclust_method = "ward.D"
```

```{r}
segmentation_attributes_used <- intersect(segmentation_attributes_used, 1:ncol(ProjectDataFactor))
profile_attributes_used <- intersect(profile_attributes_used, 1:ncol(ProjectDataFactor))

ProjectData_segment <- ProjectDataFactor_scaled[,segmentation_attributes_used]
ProjectData_profile <- ProjectDataFactor_scaled[,profile_attributes_used]
```


### 2.2 Visualize Pair-wise Distances

We will skip this subsection for our 2nd try.

### 2.3 Number of Segments

Let's plot the "distances" between clusters before we merge any of the lower and smaller sized clusters into larger ones.

```{r}
Hierarchical_Cluster_distances <- dist(ProjectData_segment, method=distance_used)
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method=hclust_method)

num <- nrow(ProjectData) - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
iplot.df(melt(head(df1, 20), id="index"), xlab="Number of Components")
```

For now, we will choose 5 segments. We can see the segment each observation (respondent in this case) belongs to for the first 20 people:

```{r}
cluster_memberships_hclust <- as.vector(cutree(Hierarchical_Cluster, 5))
cluster_ids_hclust = unique(cluster_memberships_hclust)

ProjectData_with_hclust_membership <- cbind(1:length(cluster_memberships_hclust),cluster_memberships_hclust)
colnames(ProjectData_with_hclust_membership)<-c("Observation Number","Cluster_Membership")

knitr::kable(round(head(ProjectData_with_hclust_membership, 20), 2))
```

### 2.4 Profile and interpret the segments

Having decided how many clusters to use, we would like to have a better understanding of who the customers in those clusters are and interpret the segments.

Let's see first how many observations we have in each segment, for the segments we selected above:

```{r}
cluster_memberships <- cluster_memberships_hclust
cluster_ids <-  cluster_ids_hclust

cluster_size = NULL
for (i in sort(unique(cluster_memberships))){
  cluster_size = c(cluster_size,sum(cluster_memberships == i))
  }
cluster_size = matrix(cluster_size, nrow=1)
colnames(cluster_size) <- paste("Segment", 1:length(cluster_size), sep=" ")
rownames(cluster_size) <- "Number of Obs."
iprint.df(cluster_size, scale=TRUE)
```

The average values of our data for the total population as well as within each customer segment are:

```{r}
NewData = matrix(cluster_memberships,ncol=1)

population_average = matrix(apply(ProjectData_profile, 2, mean), ncol=1)
colnames(population_average) <- "Population"
Cluster_Profile_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_profile[(cluster_memberships==i), ], 2, mean))
if (ncol(ProjectData_profile) <2)
  Cluster_Profile_mean=t(Cluster_Profile_mean)
colnames(Cluster_Profile_mean) <- paste("Segment", 1:length(cluster_ids), sep=" ")
cluster.profile <- cbind (population_average,Cluster_Profile_mean)

knitr::kable(round(cluster.profile, 2))
```

Segment 1 to 4 has quite clear differentiation among segments, and we were able to profile them as 'Quitters', 'Pampered Loyals', 'The Burned-outs', and 'Neglected Loyals'. However, everyone in Segment 5 has had a work accident (coefficient 1), which is a variable we consider not to be much meaningful in exploring who and how to retain.
Therefore, we will redo the analysis again excluding work accident as a variable.

-----

### Test #3
### 2.1 Select segmentation variables and methods

We will now use the variables except 'Whether employee has left','Department', and 'Work accident'. We will use Euclidean distance, just like before.

```{r, echo=TRUE, tidy=TRUE}
segmentation_attributes_used = c(1:5,8:9)
profile_attributes_used = c(1:19)
numb_clusters_used = 4
profile_with = "hclust"
distance_used = "euclidean"
hclust_method = "ward.D"
```

```{r}
segmentation_attributes_used <- intersect(segmentation_attributes_used, 1:ncol(ProjectDataFactor))
profile_attributes_used <- intersect(profile_attributes_used, 1:ncol(ProjectDataFactor))

ProjectData_segment <- ProjectDataFactor_scaled[,segmentation_attributes_used]
ProjectData_profile <- ProjectDataFactor_scaled[,profile_attributes_used]
```


### 2.2 Visualize Pair-wise Distances

We will skip this subsection for our 3rd try.

### 2.3 Number of Segments

Let's plot the "distances" between clusters before we merge any of the lower and smaller sized clusters into larger ones.

```{r}
Hierarchical_Cluster_distances <- dist(ProjectData_segment, method=distance_used)
Hierarchical_Cluster <- hclust(Hierarchical_Cluster_distances, method=hclust_method)

num <- nrow(ProjectData) - 1
df1 <- cbind(as.data.frame(Hierarchical_Cluster$height[length(Hierarchical_Cluster$height):1]), c(1:num))
colnames(df1) <- c("distances","index")
iplot.df(melt(head(df1, 20), id="index"), xlab="Number of Components")
```

The appropriate number of segments is 4, with the distance between clusters dropping drastically after 4. Below are the segments assigned to each employee, for the first 20 employees (observations).

```{r}
cluster_memberships_hclust <- as.vector(cutree(Hierarchical_Cluster, 4))
cluster_ids_hclust = unique(cluster_memberships_hclust)

ProjectData_with_hclust_membership <- cbind(1:length(cluster_memberships_hclust),cluster_memberships_hclust)
colnames(ProjectData_with_hclust_membership)<-c("Observation Number","Cluster_Membership")

knitr::kable(round(head(ProjectData_with_hclust_membership, 20), 2))
```

### 2.4 Profile and interpret the segments

The number and segmentation of the clustuers seem reasonable. Now, in order to get a better understanding of who the employyes in those clusters are, we will attempt to profile and interpret the segments using the all the variables (attributes) we originally had, including the variables we excluded for segmentation.

Let's first see how many observations we have in each segment, for the segments we selected above:

```{r}
cluster_memberships <- cluster_memberships_hclust
cluster_ids <-  cluster_ids_hclust

cluster_size = NULL
for (i in sort(unique(cluster_memberships))){
  cluster_size = c(cluster_size,sum(cluster_memberships == i))
  }
cluster_size = matrix(cluster_size, nrow=1)
colnames(cluster_size) <- paste("Segment", 1:length(cluster_size), sep=" ")
rownames(cluster_size) <- "Number of Obs."
iprint.df(cluster_size, scale=TRUE)
```

The average values of our data for the total population as well as within each customer segment are as below:

```{r}
NewData = matrix(cluster_memberships,ncol=1)

population_average = matrix(apply(ProjectData_profile, 2, mean), ncol=1)
colnames(population_average) <- "Population"
Cluster_Profile_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData_profile[(cluster_memberships==i), ], 2, mean))
if (ncol(ProjectData_profile) <2)
  Cluster_Profile_mean=t(Cluster_Profile_mean)
colnames(Cluster_Profile_mean) <- paste("Segment", 1:length(cluster_ids), sep=" ")
cluster.profile <- cbind (population_average,Cluster_Profile_mean)

knitr::kable(round(cluster.profile, 2))
```

After analyzing the results, we were able to define each segment:

* Segment 1 – *"revolving doors"* – low preforming with high likelihood to leave – these employees are low performers with low satisfaction levels. Although their average salary is just below company average they show very low commitment with low utilization and working hours way below average. 

* Segment 2 – *"pampered loyalists"* – high preforming with low likelihood to leave – the main characteristic of these employees is high salaries; almost double than the company average. They show very high satisfaction levels and average performance across all main parameters.   

* Segment 3 – *"Burned"* – High preforming with high likelihood to leave – these employees show very low levels of satisfaction, probably due to over utilization (above average number of hours and number of projects). The high commitment does not reflect in salaries which are below average. 

* Segment 4 – *"Happy Cash Cows"* – high preforming low likelihood to leave – these employees show tremendously high levels of satisfaction although their salaries are extremely low. They present decent performance across all main parameters and are very unlikely to leave. 
the interesting segment emerging from the analysis above is segment 3, good employees in high risk. The data revels four drivers for attrition: Low level of salary, high utilization and low levels of satisfaction. 

-----

## 3. Classification Analysis

We will also use the classification analysis methods to understand the key drivers for leaving. Hence our dependent variable is 'Whether employee has left.'

```{r setupclassification, echo=TRUE, tidy=TRUE}
dependent_variable = 7
independent_variables = c(1:5,8:9)

Probability_Threshold = 0.5

estimation_data_percent = 80
validation_data_percent = 10

random_sampling = 0

# Tree (CART) complexity control cp
CART_cp = 0.02

# the minimum size of a segment for the analysis to be done
min_segment = 100
```

```{r}
dependent_variable = unique(sapply(dependent_variable,function(i) min(ncol(ProjectDataFactor_scaled), max(i,1))))
independent_variables = unique(sapply(independent_variables,function(i) min(ncol(ProjectDataFactor_scaled), max(i,1))))

test_data_percent = 100 - estimation_data_percent - validation_data_percent
CART_control = rpart.control(cp = CART_cp)
```

```{r}
# First we split the data in estimation, validation, and test

if (random_sampling){
  estimation_data_ids=sample.int(nrow(ProjectDataFactor_scaled),floor(estimation_data_percent*nrow(ProjectDataFactor_scaled)/100))
  non_estimation_data = setdiff(1:nrow(ProjectDataFactor_scaled),estimation_data_ids)
  validation_data_ids=non_estimation_data[sample.int(length(non_estimation_data), floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))]
  } else {
    estimation_data_ids=1:floor(estimation_data_percent*nrow(ProjectDataFactor_scaled)/100)
    non_estimation_data = setdiff(1:nrow(ProjectDataFactor_scaled),estimation_data_ids)
    validation_data_ids = (tail(estimation_data_ids,1)+1):(tail(estimation_data_ids,1) + floor(validation_data_percent/(validation_data_percent+test_data_percent)*length(non_estimation_data)))
    }

test_data_ids = setdiff(1:nrow(ProjectDataFactor_scaled), union(estimation_data_ids,validation_data_ids))

estimation_data=ProjectDataFactor_scaled[estimation_data_ids,]
validation_data=ProjectDataFactor_scaled[validation_data_ids,]
test_data=ProjectDataFactor_scaled[test_data_ids,]
```

This is a “small tree” classification for example:

```{r}
independent_variables_nolabel = colnames(ProjectDataFactor[,independent_variables])

estimation_data_nolabel = cbind(estimation_data[,dependent_variable], estimation_data[,independent_variables])
colnames(estimation_data_nolabel) <- c(colnames(estimation_data)[dependent_variable],independent_variables_nolabel)

validation_data_nolabel = cbind(validation_data[,dependent_variable], validation_data[,independent_variables])
colnames(validation_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

test_data_nolabel = cbind(test_data[,dependent_variable], test_data[,independent_variables])
colnames(test_data_nolabel)<- c(dependent_variable,independent_variables_nolabel)

estimation_data_nolabel = data.frame(estimation_data_nolabel)
validation_data_nolabel = data.frame(validation_data_nolabel)
test_data_nolabel = data.frame(test_data_nolabel)

estimation_data = data.frame(estimation_data)
validation_data = data.frame(validation_data)
test_data = data.frame(test_data)

formula=paste(colnames(estimation_data)[dependent_variable],paste(Reduce(paste,sapply(head(independent_variables_nolabel,-1), function(i) paste(i,"+",sep=""))),tail(independent_variables_nolabel,1),sep=""),sep="~")
CART_tree<-rpart(formula, data=estimation_data_nolabel, method="class", control=CART_control)

rpart.plot(CART_tree, box.palette="OrBu", type=3, extra=1, fallen.leaves=F, branch.lty=3)
```

From the tree analysis we can understand 'satisfacton level' is the most important driver. This is consistent with our segmentation, in which less satisfied segments are more likely to leave.

-----

## 4. Business Decisions

To synthesize our analysis, we have three recommendations to companies.

1. We recommend companies to allocate their resources to improving satisfaction among employees in Segment 3, the “Burned” employees since this segment represents those who are high performing but are also most likely to leave the company. To retain employees in this segment, companies should:
    *increase salaries
    *decrease the number of work hours, and;
    *reduce the number of projects per employee, redirecting it to other employees.
Additional research should be done to study qualitative alternatives to improve this segment’s work satisfaction.

2. We recommend companies to gather additional employee information in order to further analyze Segment 4, such as employee function and employee seniority. This segment has significantly high employee satisfaction and performance yet have low likelihood of leaving, and further analysis should be focused on determining why this is. Our hypothesis is that Segment 4 may include a large number of junior employees or interns, who are content with learning and doing a large amount of work despite small pay. If this is so, then reproducing work conditions of Segment 4 to Segment 3 to retain “Burned” employees is not possible. Further analysis can confirm or reject this hypothesis. 

3. We recommend companies to further analyze Segment 1 to understand why employees are low performing and why they leave. Several possible drivers for low performance could be inadequate training, bad management, or inaccurate guidance for HR hiring procedures (ie. Hiring the wrong people). This understanding can help companies to reduce costs associated with high employee turnover.
